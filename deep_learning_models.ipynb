{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we implement LSTM Neural Network for our classification task, seeing whether it is capable of outperforming our traditional ML models (Random Forest, XGBoost, etc.).\n",
    "Our motivation for testing such type of model comes from its ability to capture long-term dependencies in time series data, which is a key feature of our dataset. Additionally, a large amount of papers read on the topic where aimig at assessing whether LSTM models would perform better than traditional ML models. These papers had opposing conclusions. In order to discernate, we will conduct the experiment ourselves. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Enable MPS acceleration for Apple Silicon\n",
    "tf.config.set_visible_devices([], 'GPU')  # To ensure it uses the MPS backend\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data and Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_final shape: (73271, 68)\n",
      "X_test_final shape: (18318, 68)\n",
      "y_train_encoded shape: (73271,)\n",
      "y_test shape: (18318,)\n"
     ]
    }
   ],
   "source": [
    "# import our data\n",
    "import pickle\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open(\"train_test_data.pkl\", \"rb\") as f:\n",
    "    X_train_final, X_test_final, y_train_encoded, y_test = pickle.load(f)\n",
    "\n",
    "# Check the shapes of the imported data\n",
    "print(\"X_train_final shape:\", X_train_final.shape)\n",
    "print(\"X_test_final shape:\", X_test_final.shape)\n",
    "print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution in y_train_encoded: {0: 8145, 1: 25621, 2: 39505}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check class distribution\n",
    "unique_classes, class_counts = np.unique(y_train_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(unique_classes, class_counts))\n",
    "\n",
    "print(\"Class Distribution in y_train_encoded:\", class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "Counter({2: 39505, 1: 25621, 0: 8145})\n",
      "\n",
      "Balanced class distribution:\n",
      "Counter({2: 39505, 0: 39505, 1: 39505})\n",
      "\n",
      "Balanced dataset shapes:\n",
      "Features shape: (118515, 68)\n",
      "Target shape: (118515,)\n"
     ]
    }
   ],
   "source": [
    "# This is optional as we do not need to sample the data to train but it might perform better. We will compare later with non-sampled data later. \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the dataset\n",
    "# Assuming X_train_final and y_train_encoded are your features and target\n",
    "X_train = X_train_final.copy()\n",
    "y_train = y_train_encoded.copy()\n",
    "\n",
    "print(\"Original class distribution:\")\n",
    "print(Counter(y_train))  # Check initial distribution\n",
    "\n",
    "# Step 2: Identify classes and their counts\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "max_count = counts.max()  # Count of the majority class\n",
    "\n",
    "# Step 3: Initialize placeholders for synthetic samples\n",
    "X_synthetic = []\n",
    "y_synthetic = []\n",
    "\n",
    "# Step 4: Oversample each minority class\n",
    "k_neighbors = 5  # Number of neighbors for interpolation\n",
    "for cls, count in zip(classes, counts):\n",
    "    if count < max_count:  # Only oversample classes smaller than the majority class\n",
    "        n_samples_to_generate = max_count - count  # Number of synthetic samples needed\n",
    "        X_minority = X_train[y_train == cls]\n",
    "\n",
    "        # Use NearestNeighbors to find neighbors in the minority class\n",
    "        nbrs = NearestNeighbors(n_neighbors=k_neighbors).fit(X_minority)\n",
    "        indices = nbrs.kneighbors(X_minority, return_distance=False)\n",
    "\n",
    "        # Generate synthetic samples\n",
    "        synthetic_samples = []\n",
    "        for _ in range(n_samples_to_generate):\n",
    "            idx = np.random.randint(0, X_minority.shape[0])  # Randomly pick a minority sample\n",
    "            nn_idx = indices[idx][np.random.randint(1, k_neighbors)]  # Pick one of its neighbors\n",
    "            diff = X_minority[nn_idx] - X_minority[idx]  # Difference vector\n",
    "            synthetic_sample = X_minority[idx] + np.random.rand() * diff  # Interpolation\n",
    "            synthetic_samples.append(synthetic_sample)\n",
    "\n",
    "        synthetic_samples = np.array(synthetic_samples)\n",
    "\n",
    "        # Append synthetic samples and labels to placeholders\n",
    "        X_synthetic.append(synthetic_samples)\n",
    "        y_synthetic.append(np.full(n_samples_to_generate, cls))\n",
    "\n",
    "# Step 5: Combine synthetic samples with the original dataset\n",
    "X_synthetic = np.vstack(X_synthetic) if X_synthetic else np.empty((0, X_train.shape[1]))\n",
    "y_synthetic = np.hstack(y_synthetic) if y_synthetic else np.empty((0,))\n",
    "\n",
    "X_train_balanced = np.vstack([X_train, X_synthetic])\n",
    "y_train_balanced = np.hstack([y_train, y_synthetic])\n",
    "\n",
    "# Step 6: Verify the new class distribution\n",
    "print(\"\\nBalanced class distribution:\")\n",
    "print(Counter(y_train_balanced))\n",
    "\n",
    "# Step 7: Output shapes\n",
    "print(\"\\nBalanced dataset shapes:\")\n",
    "print(f\"Features shape: {X_train_balanced.shape}\")\n",
    "print(f\"Target shape: {y_train_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled dataset shapes:\n",
      "Features shape: (118515, 68)\n",
      "Target shape: (118515,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Combine features and labels into a single dataset for shuffling\n",
    "X_train_balanced, y_train_balanced = shuffle(X_train_balanced, y_train_balanced, random_state=42)\n",
    "\n",
    "# Verify the shapes after shuffling\n",
    "print(\"Shuffled dataset shapes:\")\n",
    "print(f\"Features shape: {X_train_balanced.shape}\")\n",
    "print(f\"Target shape: {y_train_balanced.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the shapes before reshaping\n",
    "print(\"Original shapes:\")\n",
    "print(f\"Features shape: {X_train_balanced.shape}\")\n",
    "print(f\"Target shape: {y_train_balanced.shape}\")\n",
    "\n",
    "# Reshape features to add the timesteps dimension (LSTM expects 3D input)\n",
    "X_train_lstm = X_train_balanced.reshape(X_train_balanced.shape[0], 1, X_train_balanced.shape[1])  # 1 timestep\n",
    "y_train_lstm = y_train_balanced  # Target shape doesn't need to change\n",
    "\n",
    "# If you have a validation set, reshape it as well\n",
    "X_val_lstm = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])  # Reshape validation features\n",
    "\n",
    "# Verify the shapes after reshaping\n",
    "print(\"Reshaped dataset shapes:\")\n",
    "print(f\"LSTM Features shape: {X_train_lstm.shape}\")\n",
    "print(f\"Target shape: {y_train_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "model = Sequential([\n",
    "    Masking(mask_value=-1, input_shape=(X.shape[1], X.shape[2])),  # Handle padded/missing values\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(y.shape[1], activation='softmax')  # Output layer for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('lstm_readmission_model.h5')\n",
    "\n",
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model('lstm_readmission_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
